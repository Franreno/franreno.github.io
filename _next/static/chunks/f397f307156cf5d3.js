(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,55031,e=>{"use strict";var s=e.i(43476),t=e.i(71645),a=e.i(18566);function i(){let[e,i]=(0,t.useState)(!0),[n,r]=(0,t.useState)(0),l=(0,a.usePathname)();return((0,t.useEffect)(()=>{if(sessionStorage.getItem("hasVisited"))return void i(!1);let e=[{delay:500,action:()=>r(1)},{delay:1200,action:()=>r(2)},{delay:2e3,action:()=>{i(!1),sessionStorage.setItem("hasVisited","true")}}].map(({delay:e,action:s})=>setTimeout(s,e));return()=>e.forEach(clearTimeout)},[]),e)?(0,s.jsxs)("div",{style:{position:"fixed",top:0,left:0,right:0,bottom:0,background:"#0a0a0a",zIndex:9999,display:"flex",flexDirection:"column",alignItems:"center",justifyContent:"center",gap:"1rem"},children:[n>=1&&(0,s.jsx)("div",{className:"terminal-text",style:{fontSize:"1.25rem",animation:"fadeIn 0.3s ease-in"},children:"> SYSTEM.BOOT [OK]"}),n>=2&&(0,s.jsxs)("div",{className:"terminal-text",style:{fontSize:"1.25rem",animation:"fadeIn 0.3s ease-in"},children:[`> LOADING_${"/"===l?"HOME":"/projects"===l?"PROJECTS":"/research"===l?"RESEARCH":"/blog"===l?"BLOG":"PAGE"}.EXE`,(0,s.jsx)("span",{className:"blink",children:"..."})]}),n>=2&&(0,s.jsx)("div",{style:{width:"300px",height:"4px",background:"#1a1a1a",border:"2px solid #333",marginTop:"2rem",position:"relative",overflow:"hidden"},children:(0,s.jsx)("div",{style:{position:"absolute",top:0,left:0,height:"100%",width:"100%",background:"linear-gradient(90deg, #a855f7 0%, #00ff41 50%, #00ffff 100%)",animation:"loading 0.8s ease-in-out",boxShadow:"0 0 10px rgba(168, 85, 247, 0.6)"}})}),n>=2&&(0,s.jsx)("div",{className:"terminal-text",style:{fontSize:"0.875rem",marginTop:"1rem",opacity:.6,animation:"fadeIn 0.5s ease-in 1.5s both"},children:"> Initializing Y2K interface..."})]}):null}e.s(["BootSequence",()=>i])},40491,e=>{"use strict";var s=e.i(43476),t=e.i(55031);function a(){return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.BootSequence,{}),(0,s.jsxs)("div",{className:"space-y-12",children:[(0,s.jsxs)("section",{className:"py-8 text-center",children:[(0,s.jsx)("h1",{className:"text-5xl md:text-6xl font-bold mb-6 chrome-text",children:"RESEARCH"}),(0,s.jsx)("p",{className:"text-xl",style:{color:"#b0b0b0"},children:"Multilingual AI // Computer Vision // NLP"})]}),(0,s.jsx)("hr",{className:"divider-glow"}),(0,s.jsxs)("section",{className:"y2k-box",style:{borderWidth:"2px"},children:[(0,s.jsxs)("div",{className:"flex items-start gap-4 mb-6",children:[(0,s.jsx)("span",{className:"text-4xl",children:"ðŸ”¬"}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h2",{className:"text-3xl font-bold mb-2 text-accent-bright",children:"Multilingual Referring Expression Comprehension"}),(0,s.jsx)("p",{className:"text-sm text-cyan",children:"Master's Thesis â€¢ Instituto Superior TÃ©cnico, Lisbon â€¢ 2024-2025"})]})]}),(0,s.jsxs)("div",{className:"mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-bold mb-4 text-accent",children:"Overview"}),(0,s.jsx)("p",{className:"text-base leading-relaxed mb-4",style:{color:"#b0b0b0"},children:"This research addresses a significant gap in multilingual referring expression comprehension by developing AI systems that can localize objects in images based on natural language descriptions across multiple languages."}),(0,s.jsx)("p",{className:"text-base leading-relaxed",style:{color:"#b0b0b0"},children:"The project demonstrates that effective multilingual referring expression comprehension can be achieved through strategic dataset expansion and architecture design, enabling more inclusive AI systems accessible to non-English speakers worldwide."})]}),(0,s.jsxs)("div",{className:"mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-bold mb-4 text-accent",children:"Key Contributions"}),(0,s.jsxs)("ul",{className:"space-y-3",children:[(0,s.jsxs)("li",{className:"flex items-start gap-3",children:[(0,s.jsx)("span",{className:"terminal-text font-bold mt-1",children:">"}),(0,s.jsxs)("div",{children:[(0,s.jsx)("strong",{className:"text-accent-bright",children:"Multilingual Dataset"}),(0,s.jsxs)("p",{className:"text-sm mt-1",style:{color:"#b0b0b0"},children:["Unified corpus spanning ",(0,s.jsx)("strong",{children:"10 languages"}),": English, Portuguese, Spanish, French, German, Dutch, Italian, Korean, Chinese, and Russian. Contains"," ",(0,s.jsx)("strong",{children:"8 million referring expressions"}),", ",(0,s.jsx)("strong",{children:"70,000 images"}),", and ",(0,s.jsx)("strong",{children:"346,000 annotated objects"}),". Built by expanding 12 existing English benchmarks."]})]})]}),(0,s.jsxs)("li",{className:"flex items-start gap-3",children:[(0,s.jsx)("span",{className:"terminal-text font-bold mt-1",children:">"}),(0,s.jsxs)("div",{children:[(0,s.jsx)("strong",{className:"text-accent-bright",children:"Neural Architecture"}),(0,s.jsx)("p",{className:"text-sm mt-1",style:{color:"#b0b0b0"},children:"Attention-anchored approach using frozen multilingual SigLIP2 encoders. Generates spatial anchors from attention distributions, refined through learned residuals for precise object localization."})]})]}),(0,s.jsxs)("li",{className:"flex items-start gap-3",children:[(0,s.jsx)("span",{className:"terminal-text font-bold mt-1",children:">"}),(0,s.jsxs)("div",{children:[(0,s.jsx)("strong",{className:"text-accent-bright",children:"Comprehensive Evaluation"}),(0,s.jsx)("p",{className:"text-sm mt-1",style:{color:"#b0b0b0"},children:"Designed evaluation pipeline measuring model performance across languages and metrics, enabling systematic analysis of cross-lingual capabilities."})]})]}),(0,s.jsxs)("li",{className:"flex items-start gap-3",children:[(0,s.jsx)("span",{className:"terminal-text font-bold mt-1",children:">"}),(0,s.jsxs)("div",{children:[(0,s.jsx)("strong",{className:"text-accent-bright",children:"Open Resources"}),(0,s.jsx)("p",{className:"text-sm mt-1",style:{color:"#b0b0b0"},children:"Published complete dataset, model weights, and evaluation code for community use on GitHub and Hugging Face platforms."})]})]})]})]}),(0,s.jsxs)("div",{className:"mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-bold mb-4 text-accent",children:"Performance Metrics"}),(0,s.jsxs)("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-4",children:[(0,s.jsxs)("div",{className:"p-4 text-center",style:{background:"#0f0f0f",border:"2px solid #333",boxShadow:"inset 0 0 12px rgba(168, 85, 247, 0.2)"},children:[(0,s.jsx)("div",{className:"text-3xl font-bold text-accent-bright mb-2 glow-pulse",children:"86.9%"}),(0,s.jsx)("div",{className:"text-xs uppercase tracking-wide terminal-text",children:"Accuracy at IoU@50"}),(0,s.jsx)("div",{className:"text-xs mt-1",style:{color:"#666"},children:"RefCOCO Multilingual"})]}),(0,s.jsxs)("div",{className:"p-4 text-center",style:{background:"#0f0f0f",border:"2px solid #333",boxShadow:"inset 0 0 12px rgba(168, 85, 247, 0.2)"},children:[(0,s.jsx)("div",{className:"text-3xl font-bold text-accent-bright mb-2 glow-pulse",children:"2-4%"}),(0,s.jsx)("div",{className:"text-xs uppercase tracking-wide terminal-text",children:"Gap from English"}),(0,s.jsx)("div",{className:"text-xs mt-1",style:{color:"#666"},children:"Romance Languages"})]}),(0,s.jsxs)("div",{className:"p-4 text-center",style:{background:"#0f0f0f",border:"2px solid #333",boxShadow:"inset 0 0 12px rgba(168, 85, 247, 0.2)"},children:[(0,s.jsx)("div",{className:"text-3xl font-bold text-accent-bright mb-2 glow-pulse",children:"<8%"}),(0,s.jsx)("div",{className:"text-xs uppercase tracking-wide terminal-text",children:"Performance Variance"}),(0,s.jsx)("div",{className:"text-xs mt-1",style:{color:"#666"},children:"Across Language Families"})]})]})]}),(0,s.jsxs)("div",{className:"mb-6",children:[(0,s.jsx)("h3",{className:"text-xl font-bold mb-4 text-accent",children:"Available Resources"}),(0,s.jsxs)("div",{className:"flex flex-wrap gap-3",children:[(0,s.jsx)("a",{href:"https://multilingual.franreno.com",target:"_blank",rel:"noopener noreferrer",className:"y2k-button",children:"ðŸŒ Project Website"}),(0,s.jsx)("a",{href:"https://github.com/franreno/MultilingualReferringExpression-pub",target:"_blank",rel:"noopener noreferrer",className:"y2k-button",children:"ðŸ’¾ GitHub Repository"}),(0,s.jsx)("a",{href:"https://huggingface.co/datasets/Franreno/MultilingualReferringExpression",target:"_blank",rel:"noopener noreferrer",className:"y2k-button",children:"ðŸ¤— Hugging Face"})]})]}),(0,s.jsxs)("div",{className:"mt-8 pt-6 border-t-2 border-accent",children:[(0,s.jsx)("h3",{className:"text-lg font-bold mb-4 text-accent",children:"Publication"}),(0,s.jsxs)("div",{className:"mb-4",children:[(0,s.jsxs)("div",{className:"flex items-center justify-between mb-2",children:[(0,s.jsx)("h4",{className:"text-sm font-bold text-accent-bright",children:"APA Citation"}),(0,s.jsx)("button",{onClick:()=>{navigator.clipboard.writeText("Nogueira, F. R. (2025). Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs. arXiv preprint arXiv:2511.11427.");let e=document.getElementById("apa-copy-btn");e&&(e.textContent="âœ“ COPIED",setTimeout(()=>e.textContent="ðŸ“‹ COPY",2e3))},id:"apa-copy-btn",className:"y2k-button text-xs py-1 px-3 min-w-0",style:{fontSize:"0.7rem",padding:"0.4rem 0.75rem"},children:"ðŸ“‹ COPY"})]}),(0,s.jsxs)("div",{className:"p-4 font-mono text-sm",style:{background:"#0a0a0a",border:"2px solid #333",color:"#b0b0b0"},children:[(0,s.jsxs)("p",{className:"mb-2",children:[(0,s.jsx)("strong",{className:"text-accent-bright",children:"Nogueira, F. R."})," (2025)."]}),(0,s.jsx)("p",{className:"italic mb-2",children:"Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs."}),(0,s.jsx)("p",{className:"text-xs",children:"arXiv preprint arXiv:2511.11427."})]})]}),(0,s.jsxs)("div",{children:[(0,s.jsxs)("div",{className:"flex items-center justify-between mb-2",children:[(0,s.jsx)("h4",{className:"text-sm font-bold text-accent-bright",children:"BibTeX"}),(0,s.jsx)("button",{onClick:()=>{let e=`@misc{nogueira2025comprehensionmultilingualexpressionsreferring,
      title={Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs},
      author={Francisco Nogueira and Alexandre Bernardino and Bruno Martins},
      year={2025},
      eprint={2511.11427},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2511.11427},
}`;navigator.clipboard.writeText(e);let s=document.getElementById("bibtex-copy-btn");s&&(s.textContent="âœ“ COPIED",setTimeout(()=>s.textContent="ðŸ“‹ COPY",2e3))},id:"bibtex-copy-btn",className:"y2k-button text-xs py-1 px-3 min-w-0",style:{fontSize:"0.7rem",padding:"0.4rem 0.75rem"},children:"ðŸ“‹ COPY"})]}),(0,s.jsx)("div",{className:"p-4 font-mono text-xs overflow-x-auto",style:{background:"#0a0a0a",border:"2px solid #333",color:"#00ff41"},children:(0,s.jsx)("pre",{style:{margin:0,whiteSpace:"pre"},children:`@misc{nogueira2025comprehensionmultilingualexpressionsreferring,
      title={Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs},
      author={Francisco Nogueira and Alexandre Bernardino and Bruno Martins},
      year={2025},
      eprint={2511.11427},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2511.11427},
}`})})]}),(0,s.jsx)("div",{className:"mt-4 text-center",children:(0,s.jsx)("a",{href:"https://arxiv.org/abs/2511.11427",target:"_blank",rel:"noopener noreferrer",className:"y2k-button inline-block",children:"ðŸ“„ View on arXiv"})})]})]}),(0,s.jsxs)("section",{className:"y2k-box",style:{borderWidth:"2px"},children:[(0,s.jsx)("h2",{className:"text-2xl font-bold mb-6 text-accent",children:"Research Interests"}),(0,s.jsxs)("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"text-base font-bold mb-2 text-accent-bright",children:"Multilingual NLP"}),(0,s.jsx)("p",{className:"text-sm",style:{color:"#b0b0b0"},children:"Developing AI systems that work across languages, enabling more inclusive and accessible technology for diverse global communities."})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"text-base font-bold mb-2 text-accent-bright",children:"Computer Vision"}),(0,s.jsx)("p",{className:"text-sm",style:{color:"#b0b0b0"},children:"Developing vision systems that bridge language and visual understanding, enabling machines to interpret and reason about visual content through natural language interactions."})]})]})]}),(0,s.jsxs)("section",{className:"y2k-box",style:{borderWidth:"2px"},children:[(0,s.jsx)("h2",{className:"text-2xl font-bold mb-6 text-accent",children:"Previous Research Experience"}),(0,s.jsx)("div",{className:"space-y-4",children:(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-lg font-bold text-accent-bright mb-1",children:"Research Assistant â€“ Data Analysis"}),(0,s.jsx)("p",{className:"text-sm text-cyan mb-2",children:"Universidade de SÃ£o Paulo & Universidade do Rio de Janeiro â€¢ Aug 2021 - Aug 2022"}),(0,s.jsx)("p",{className:"text-sm",style:{color:"#b0b0b0"},children:"Applied topological data analysis to multivariate biological datasets for epidemiological research. Developed data visualizations and analysis pipelines for understanding complex biological patterns."})]})})]})]})]})}e.s(["default",()=>a])}]);